%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Matrix factorization}\label{ch:mf}
In this Chapter we present matrix factorization models used in recommender systems 
and their relation to singular vector decomposition. 

\section{Model} \label{sec:model}

As it was mentioned in Chapter \ref{ch:rec_sys}, our goal is to estimate the missing element of the 
score matrix $S\in\mathbb{R}^{n\times m}$ with the product of two dense, low dimension 
matrix. Let these two matrices be $P\in\mathbb{R}^{n\times k}$ and 
$Q\in\mathbb{R}^{k\times m}$ and denote the approximation of matrix $S$ with 
$\hat{S}=PQ$.

Our prediction for a given user $u$ and item $i$ pair is:
\[\hat{s}_{ui}=p_{u}q_{i},\]

where $p_u$ and $q_i$, denote $u^{th}$ row  of $P$ and  $i^{th}$ column of $Q$
respectively. We call matrix $P$ user factors because each row  of the $P$ belongs 
to a user and similarly matrix $Q$ is called item factors. 

\section{Optimizing the factorization}
In general, we measure the quality of the approximation with a loss function $L$. 
A loss function maps model parameters to a real number. In case of matrix factorization, these 
parameter values are the factor matrices.

A frequently used loss function is the mean square error on the known $s_{u,i}$ values 
of the matrix $S$,
\begin{equation}\label{eq:mse}
  L(S,P;Q)=\sum_{(u,i)}(s_{u,i}-p_{u}q_{i})^2.
\end{equation}


Our objective is to minimize the loss function $L$ with a learning algorithm.

\subsection{Learning with stochastic gradient descent}
Stohastic gradient descent assumes that $L$ is a differentiable function \footnote{
This function is mainly MSE}. The concept behind SGD is the following:
\begin{itemize}
\item Starting from the actual model parameters $\hat{\theta}$, we determine the 
gradient \footnote{Gradient is the steepest direction in the given point.} of 
the loss function $L$ with respect to $\hat{\theta}$
\[\nabla L = \frac{\partial L}{\partial \hat{\theta}}.\]

\item Using the gradient vector, we update the parameters by moving towards the 
opposite of the gradient,

\[ \hat{\theta}_{n+1}=\hat{\theta}_n - \mu \nabla L ,\]

where $\mu$ is the \emph{learning rate}, a parameter of the algorithm. 
\end{itemize}

Applying this idea to improve the prediction for matrix $S$:

\begin{algorithm}[H]
  \KwData{Elements of matrix $S$}
  \KwResult{Factor matrices $P$ and $Q$}
    Initialize $P$ and $Q$ matrices with random values\;
  \While{termination condition($L(S,P_n;Q_n)$)}{
      Select a random $S_{ui}$\;
      Compute $\frac{\partial L}{\partial \hat{p}_{u}}$\;
      Compute $\frac{\partial L}{\partial \hat{q}_{i}}$ \;
      $\hat{p}_{u}\leftarrow \hat{p}_{u}-\mu \frac{\partial}{\partial \hat{p}_{u}} L$\;
      $\hat{q}_{u}\leftarrow \hat{q}_{u}-\mu \frac{\partial}{\partial \hat{q}_{u}} L$\;
  }
  \Return $P,Q$
  \caption{SGD to factorize matrices}
\end{algorithm}


In general, loss functions assembled from a score error minimalization part and 
a model complexity regularization part. If we would use only the score error 
minimalization part, the model may overfit.

Frequently use loss function in recommender systems is 

\[L(S,P;Q)=\sum_{(u,i)}(s_{ui}-p_u q_i)^2 + \lambda (\|p_u\|^2+\|q_u\|^2).\]

This loss function minimizes the difference between the recommended and the actual score in MSE 
and the squared sum of the length of the factors. $\lambda$ is a nonnegative, finite
parameter of the function, the regularization rate. 

In case of the above detailed loss function, the algorithm iterative steps of the algorithm 
becomes the following

\[ \hat{p}_{u}\leftarrow \hat{p}_{u}+\mu \left(2\left(s_{ui}-p_u q_i\right) q_i - \lambda p_u\right),\]
\[\hat{q}_{i}\leftarrow \hat{q}_{i}+\mu \left(2\right(s_{ui}-p_u q_i\left)p_u - \lambda q_i\right) .\]

Note that when we update $q_i$, we use the $p_u$ from the previous iteration, instead of
the previous step.



\subsection{Matrix factorization with alternating least squares}

In this part we introduce another iterative approach to factorize a matrix in order to predict by
the mentioned model in Section \ref{sec:model}. We use an objective function slightly different
from \eqref{eq:mse}, called the \emph{wighted} mean square error:
\begin{equation} \label{eq:wmse}
   L_{w}(S,P;Q)=\sum_{(u,i)}c_{ui}(s_{ui}-p_{u}q_{i})^2, 
\end{equation}

where $c_{ui}\geq0$. We can set the weight  $c_{ui}$ of each user-item pair in the objective function.
In the implicit case, we can distinguish the positive and negative events by weighting them with $c_{ui}=c^+$ and $c_{ui}=c^-$ respectively. In practice, usually $c^{+} \gg c^{-}$. 

\begin{figure}[]
\begin{algorithm}[H]\label{alg:als}
    Fill $Q_0$ with small random values. \\
    \For{$i$ in $1:N$}{
        Compute the $P_i$ matrix that minimizes $L_{w}(S,P_i;Q_{i-1})$\\
        Compute the $Q_i$ matrix that minimizes $L_{w}(S,P_i;Q_i)$
    }
  \Return $P,Q$
  \caption{ALS to factorize matrices}
\end{algorithm}
\end{figure}

 We detail the algorithm \cite{takacs2012alternating} in Algorithm~\ref{alg:als}. The main idea
 of  the ALS algorithm is to compute the best $P$ matrix for the previously set $Q$ matrix  and
 than $Q$ for $P$ to minimize the loss function and repeat this an alternating way using gradient
 method. ALS is very efficiently implementable and parallelizable to understand this we have to analyze the derivative of objective function \eqref{eq:wmse}.

\[
\frac{\partial L_{w}(S,P;Q)}{\partial p_u} = \sum_{i\in \mathcal{I}} c_{ui}\left(q_ip_u- s_{ui}\right)q_i =
\underbracket{\left(\sum_{i\in\mathcal{I}}c_{ui}q_iq_i^T\right)}_{A_{u}}p_u-
\underbracket{\sum_{i\in\mathcal{I}}c_{ui}s_{ui}q_i}_{b_u}
\]

Our goal is to minimize $L_{w}(S,P;Q)$ in $P$, that implies 
$\frac{\partial L_{w}(S,P;Q)}{\partial p_u}=0$, what is equivalent to solving the $A_{u}p_{u}=b_{u}$ equation. We can notice that $b_u$ is actually runs over the user dependent items, because for other items  $s_{ui}=0$. Furthermore, we can separate the $A_u$ sum over items into two parts by their dependence to the user:

\[A_{u}=\underbracket{\sum_{i\in \mathcal{I}}c^-q_i q_i^T}_{A^-} +
\underbracket{\sum_{i\in\mathcal{I}_{u}}(c^+-c^-)q_i q_i^T}_{A_{u}^{+}} \]

Notice that the first part contains  elements for all user and the seconds only the user
dependent ones. We can parallelize the $A^{+}_{u}$ part because the $q_i$
vectors are independent. We also can save computation complexity by computing only once $A^-$ 
for one iteration, because its value is the same for each user. 
We can say the same argumentation for optimizing $L_{w}$ in $Q$.
